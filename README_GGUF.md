# ğŸ‰ AutoGLM GGUF æ”¯æŒ

**ä¸–ç•Œé¦–ä¸ª GLM-4V è§†è§‰è¯­è¨€æ¨¡å‹çš„ GGUF æ ¼å¼å®ç°**

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.10+-green.svg)](https://www.python.org/)
[![CUDA](https://img.shields.io/badge/CUDA-12.4+-orange.svg)](https://developer.nvidia.com/cuda-toolkit)

## ğŸŒŸ é¡¹ç›®äº®ç‚¹

- âœ… **å®Œæ•´çš„ GLM-4V â†’ GGUF è½¬æ¢**ï¼šè§†è§‰ç¼–ç å™¨ + è¯­è¨€æ¨¡å‹
- âœ… **llama.cpp å¤šæ¨¡æ€é›†æˆ**ï¼šåŸç”Ÿæ”¯æŒå›¾åƒ+æ–‡æœ¬è¾“å…¥
- âœ… **æ‰‹æœºè‡ªåŠ¨åŒ–æ§åˆ¶**ï¼šå®æ—¶æˆªå›¾åˆ†æ + ADB å‘½ä»¤æ‰§è¡Œ
- âœ… **16K ä¸Šä¸‹æ–‡çª—å£**ï¼šæ”¯æŒé•¿å¯¹è¯å’Œå¤æ‚ä»»åŠ¡
- âœ… **é«˜æ€§èƒ½æ¨ç†**ï¼šGPU å…¨åŠ é€Ÿï¼Œ~37 tokens/sec

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 5 åˆ†é’Ÿä¸Šæ‰‹

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/Luckybalabala/llama-cpp-AutoGLM.git
cd llama-cpp-AutoGLM

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. å¯åŠ¨æœåŠ¡å™¨ï¼ˆæ–°ç»ˆç«¯ï¼‰
cd llama-cpp-bin
.\llama-server.exe --model ../models/converted/AutoGLM-Phone-9B-Q4_K_S.gguf \
  --mmproj ../models/converted/AutoGLM-Phone-9B-mmproj.gguf \
  --port 8080 --ctx-size 16384 --n-gpu-layers 99

# 4. æ‰§è¡Œä»»åŠ¡
python main.py --base-url http://localhost:8080/v1 "æ‰“å¼€å¾®ä¿¡"
```

è¯¦ç»†æ•™ç¨‹ï¼š[å¿«é€Ÿå¼€å§‹æŒ‡å—](QUICK_START_GGUF.md)

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| **æ¨¡å‹å¤§å°** | 5.35GB (Q4_K_S) |
| **mmproj å¤§å°** | 1.55GB |
| **ä¸Šä¸‹æ–‡é•¿åº¦** | 16,384 tokens |
| **GPU åŠ é€Ÿ** | 41/41 å±‚ |
| **æ¨ç†é€Ÿåº¦** | ~37 tokens/sec |
| **å¹³å‡ä»»åŠ¡æ—¶é•¿** | 3-10ç§’ |

æµ‹è¯•ç¯å¢ƒï¼šRTX 4090, 32GB RAM, CUDA 12.4

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### 1. GGUF æ¨¡å‹è½¬æ¢

```bash
# è¯­è¨€æ¨¡å‹è½¬æ¢
python llama.cpp/convert_hf_to_gguf.py models/AutoGLM-Phone-9B \
    --outtype q4_k_s \
    --outfile models/converted/AutoGLM-Phone-9B-Q4_K_S.gguf

# è§†è§‰ç¼–ç å™¨è½¬æ¢
.\generate_mmproj_simple.ps1
```

æ”¯æŒçš„é‡åŒ–æ ¼å¼ï¼šQ4_K_S, Q4_K_M, Q5_K_S, Q8_0, F16

### 2. å¤šæ¨¡æ€æ¨ç†

```python
from phone_agent.model import ModelClient

client = ModelClient("http://localhost:8080/v1")
response = client.request(
    text="è¿™æ˜¯ä»€ä¹ˆï¼Ÿ",
    image_base64=screenshot_b64
)
```

### 3. æ‰‹æœºè‡ªåŠ¨åŒ–

```bash
# æ‰“å¼€åº”ç”¨
python main.py --base-url http://localhost:8080/v1 "æ‰“å¼€å¾®ä¿¡"

# å¤æ‚ä»»åŠ¡
python main.py --base-url http://localhost:8080/v1 \
  "åœ¨å¾®ä¿¡æœç´¢autoGLMç¾¤å¹¶å‘é€æ¶ˆæ¯ï¼šæµ‹è¯•æˆåŠŸ"
```

æ”¯æŒæ“ä½œï¼šLaunch, Tap, Type, Swipe, Back, Home

## ğŸ”§ æŠ€æœ¯æ¶æ„

### æ ¸å¿ƒçªç ´

#### 1. ç»´åº¦è½¬æ¢
```
GLM-4V è¾“å‡º: [HÃ—W, 6144]
    â†“ é™ç»´
llama.cpp æœŸæœ›: [HÃ—W, 4096]
```

#### 2. å±‚æ˜ å°„ä¿®å¤
```python
# GLM-4V merger ç»“æ„
{
    "norm": "mlp.0",      # Layer normalization
    "up_proj": "mlp.1",   # Up projection
    "down_proj": "mlp.3"  # Down projection
}
```

#### 3. å¯é€‰ CLS Token
```cpp
if (model.class_embedding != nullptr) {
    // æ·»åŠ  CLS token
    embeddings = ggml_concat(ctx, class_emb, embeddings, 1);
}
```

### å…³é”®æ–‡ä»¶ä¿®æ”¹

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ | çŠ¶æ€ |
|------|---------|------|
| `llama.cpp/tools/mtmd/clip.cpp` | InternVL æ¶æ„é€‚é… | âœ… |
| `llama.cpp/convert_hf_to_gguf.py` | GLM-4V è½¬æ¢æ”¯æŒ | âœ… |
| `phone_agent/model/client.py` | API å…¼å®¹æ€§æ”¹è¿› | âœ… |

## ğŸ“š æ–‡æ¡£

- ğŸ“– [å¿«é€Ÿå¼€å§‹](QUICK_START_GGUF.md) - 5åˆ†é’Ÿä¸Šæ‰‹æŒ‡å—
- ğŸ“ [å®Œæ•´æŠ€æœ¯æ–‡æ¡£](GLM4V_GGUF_COMPLETE.md) - æ·±å…¥æŠ€æœ¯ç»†èŠ‚
- ğŸ”„ [æ›´æ–°æ—¥å¿—](CHANGELOG_GGUF.md) - ç‰ˆæœ¬å˜æ›´è®°å½•
- ğŸ› ï¸ [mmproj ç”ŸæˆæŒ‡å—](MMPROJ_GENERATION_GUIDE.md) - è½¬æ¢æ•™ç¨‹
- ğŸ¤ [è´¡çŒ®æŒ‡å—](CONTRIBUTING_GGUF.md) - å‚ä¸å¼€å‘

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ç”¨æ³•

```python
# ç¤ºä¾‹ 1: æ‰“å¼€åº”ç”¨
python main.py --base-url http://localhost:8080/v1 "æ‰“å¼€å¾®ä¿¡"

# ç¤ºä¾‹ 2: å¯¼èˆªæ“ä½œ
python main.py --base-url http://localhost:8080/v1 "è¿”å›ä¸»å±å¹•"

# ç¤ºä¾‹ 3: æœç´¢å’Œå‘é€
python main.py --base-url http://localhost:8080/v1 \
  "åœ¨å¾®ä¿¡æœç´¢autoGLMå¹¶å‘é€ï¼šä½ å¥½"
```

### é«˜çº§ç”¨æ³•

```python
from phone_agent import PhoneAgent
from phone_agent.model import ModelConfig

# è‡ªå®šä¹‰é…ç½®
config = ModelConfig(
    base_url="http://localhost:8080/v1",
    model_name="AutoGLM-Phone-9B",
    temperature=0.0,
    max_tokens=500
)

agent = PhoneAgent(config)
result = agent.execute_task("ä½ çš„ä»»åŠ¡")
```

## ğŸ” æŠ€æœ¯ç»†èŠ‚

### æ•°æ®æµ

```
ç”¨æˆ·ä»»åŠ¡
   â†“
å®æ—¶æˆªå›¾ (ADB)
   â†“
Base64 ç¼–ç 
   â†“
Vision Encoder (mmproj)
   â†“ [HÃ—W, 6144]
ç»´åº¦é™ç»´
   â†“ [HÃ—W, 4096]
Language Model
   â†“
<think>åˆ†æ</think>
<answer>æ“ä½œ</answer>
   â†“
è§£æ + æ‰§è¡Œ (ADB)
   â†“
ç»“æœåé¦ˆ
```

### å…³é”®æŠ€æœ¯

**1. è§†è§‰ç¼–ç **
- EVA-CLIP-6B ä½œä¸ºè§†è§‰ç¼–ç å™¨
- Pixel unshuffle ç©ºé—´ä¸‹é‡‡æ ·
- å¯å­¦ä¹ çš„ä½ç½®ç¼–ç 

**2. è§†è§‰-è¯­è¨€å¯¹é½**
- MLP merger é™ç»´æŠ•å½±
- FFN (SwiGLU) æ¿€æ´»å‡½æ•°
- æ®‹å·®è¿æ¥ + Layer Norm

**3. æ¨ç†ä¼˜åŒ–**
- CUDA å›¾ä¼˜åŒ–
- Flash Attention
- KV Cache ç®¡ç†

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: llama-server å¯åŠ¨å¤±è´¥**
```bash
# æ£€æŸ¥ CUDA
nvidia-smi

# é‡æ–°ç¼–è¯‘
cd llama.cpp
.\rebuild_llama_server.ps1
```

**Q: ä¸Šä¸‹æ–‡æº¢å‡º**
```bash
# å¢åŠ ä¸Šä¸‹æ–‡
--ctx-size 16384  # æˆ–æ›´å¤§
```

**Q: ADB è¿æ¥å¤±è´¥**
```bash
# é‡å¯ ADB
adb kill-server
adb start-server

# æ— çº¿è¿æ¥
adb connect 192.168.x.x:port
```

æ›´å¤šé—®é¢˜ï¼š[å¿«é€Ÿå¼€å§‹æŒ‡å— - å¸¸è§é—®é¢˜](QUICK_START_GGUF.md#å¸¸è§é—®é¢˜)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿å„ç§å½¢å¼çš„è´¡çŒ®ï¼

- ğŸ› æŠ¥å‘Š Bug
- ğŸ’¡ æå‡ºåŠŸèƒ½å»ºè®®
- ğŸ“ æ”¹è¿›æ–‡æ¡£
- ğŸ”§ æäº¤ä»£ç 

æŸ¥çœ‹ [è´¡çŒ®æŒ‡å—](CONTRIBUTING_GGUF.md) äº†è§£è¯¦æƒ…ã€‚

### è´¡çŒ®è€…

æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT è®¸å¯è¯](LICENSE)ã€‚

## ğŸ™ è‡´è°¢

- [llama.cpp](https://github.com/ggerganov/llama.cpp) - ä¼˜ç§€çš„æ¨ç†æ¡†æ¶
- [AutoGLM](https://github.com/THUDM/AutoGLM) - åŸå§‹æ¨¡å‹
- [GLM-4V](https://github.com/THUDM/GLM-4) - è§†è§‰è¯­è¨€æ¨¡å‹

## ğŸ“ è”ç³»æ–¹å¼

- **Issues**: [GitHub Issues](https://github.com/Luckybalabala/llama-cpp-AutoGLM/issues)
- **Discussions**: [GitHub Discussions](https://github.com/Luckybalabala/llama-cpp-AutoGLM/discussions)
- **å¾®ä¿¡ç¾¤**: [åŠ å…¥è®¨è®º](resources/WECHAT.md)

## ğŸ—ºï¸ è·¯çº¿å›¾

### å·²å®Œæˆ âœ…
- [x] GLM-4V GGUF è½¬æ¢
- [x] llama.cpp é›†æˆ
- [x] æ‰‹æœºè‡ªåŠ¨åŒ–åŸºç¡€åŠŸèƒ½
- [x] 16K ä¸Šä¸‹æ–‡æ”¯æŒ

### è¿›è¡Œä¸­ ğŸš§
- [ ] UI å¯¼èˆªä¼˜åŒ–
- [ ] ä¸Šä¸‹æ–‡è‡ªåŠ¨ç®¡ç†
- [ ] æ›´å¤šåº”ç”¨æ”¯æŒ

### è®¡åˆ’ä¸­ ğŸ“‹
- [ ] Web UI æ”¹è¿›
- [ ] æ‰¹é‡ä»»åŠ¡å¤„ç†
- [ ] æ€§èƒ½ç›‘æ§
- [ ] æ›´å¤šè¯­è¨€æ”¯æŒ

## ğŸ“ˆ é¡¹ç›®çŠ¶æ€

![GitHub stars](https://img.shields.io/github/stars/Luckybalabala/llama-cpp-AutoGLM?style=social)
![GitHub forks](https://img.shields.io/github/forks/Luckybalabala/llama-cpp-AutoGLM?style=social)
![GitHub issues](https://img.shields.io/github/issues/Luckybalabala/llama-cpp-AutoGLM)

---

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼**

**ğŸ‰ è¿™æ˜¯ä¸–ç•Œé¦–ä¸ª GLM-4V GGUF å®ç°ï¼Œè®©æˆ‘ä»¬ä¸€èµ·æ¨åŠ¨å¼€æºå¤šæ¨¡æ€ AI çš„å‘å±•ï¼**
