# ğŸ‰ AutoGLM GGUF æ”¯æŒ

**ä¸–ç•Œé¦–ä¸ª GLM-4V è§†è§‰è¯­è¨€æ¨¡å‹çš„ GGUF æ ¼å¼å®ç°**

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.10+-green.svg)](https://www.python.org/)
[![CUDA](https://img.shields.io/badge/CUDA-12.4+-orange.svg)](https://developer.nvidia.com/cuda-toolkit)

## ğŸŒŸ é¡¹ç›®äº®ç‚¹

- âœ… **å®Œæ•´çš„ GLM-4V â†’ GGUF è½¬æ¢**ï¼šè§†è§‰ç¼–ç å™¨ + è¯­è¨€æ¨¡å‹
- âœ… **llama.cpp å¤šæ¨¡æ€é›†æˆ**ï¼šåŸç”Ÿæ”¯æŒå›¾åƒ+æ–‡æœ¬è¾“å…¥
- âœ… **æ‰‹æœºè‡ªåŠ¨åŒ–æ§åˆ¶**ï¼šå®æ—¶æˆªå›¾åˆ†æ + ADB å‘½ä»¤æ‰§è¡Œ
- âœ… **16K ä¸Šä¸‹æ–‡çª—å£**ï¼šæ”¯æŒé•¿å¯¹è¯å’Œå¤æ‚ä»»åŠ¡
- âœ… **é«˜æ€§èƒ½æ¨ç†**ï¼šGPU å…¨åŠ é€Ÿï¼Œ~37 tokens/sec

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¼–è¯‘ llama.cpp

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/Luckybalabala/llama-cpp-AutoGLM.git
cd llama-cpp-AutoGLM

# 2. ç¼–è¯‘ï¼ˆWindows + CUDAï¼‰
mkdir build
cd build
cmake .. -DGGML_CUDA=ON
cmake --build . --config Release

# 3. è½¬æ¢ GLM-4V æ¨¡å‹
python convert_hf_to_gguf.py /path/to/AutoGLM-Phone-9B --outtype q4_k_s
python convert_hf_to_gguf.py /path/to/AutoGLM-Phone-9B --mmproj --outtype f16

# 4. å¯åŠ¨æœåŠ¡å™¨
.\build\bin\Release\llama-server.exe \
  --model AutoGLM-Phone-9B-Q4_K_S.gguf \
  --mmproj mmproj-AutoGLM-Phone-9B-f16.gguf \
  --port 8080 --ctx-size 16384 --n-gpu-layers 99
```

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| **æ¨¡å‹å¤§å°** | 5.35GB (Q4_K_S) |
| **mmproj å¤§å°** | 1.55GB |
| **ä¸Šä¸‹æ–‡é•¿åº¦** | 16,384 tokens |
| **GPU åŠ é€Ÿ** | 41/41 å±‚ |
| **æ¨ç†é€Ÿåº¦** | ~37 tokens/sec |
| **å¹³å‡ä»»åŠ¡æ—¶é•¿** | 3-10ç§’ |

æµ‹è¯•ç¯å¢ƒï¼šRTX 4060 8GB, 32GB RAM, CUDA 12.4

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### 1. GLM-4V GGUF è½¬æ¢æ”¯æŒ

æœ¬ fork æ·»åŠ äº†å®Œæ•´çš„ GLM-4V/AutoGLM æ¨¡å‹è½¬æ¢æ”¯æŒï¼š

```bash
# è¯­è¨€æ¨¡å‹è½¬æ¢
python convert_hf_to_gguf.py /path/to/AutoGLM-Phone-9B \
    --outtype q4_k_s \
    --outfile AutoGLM-Phone-9B-Q4_K_S.gguf

# è§†è§‰ç¼–ç å™¨è½¬æ¢ï¼ˆmmprojï¼‰
python convert_hf_to_gguf.py /path/to/AutoGLM-Phone-9B \
    --mmproj \
    --outtype f16
```

**æ”¯æŒçš„é‡åŒ–æ ¼å¼**ï¼šQ4_K_S, Q4_K_M, Q5_K_S, Q8_0, F16

### 2. å¤šæ¨¡æ€æ¨ç†

ä½¿ç”¨ OpenAI å…¼å®¹ APIï¼š

```python
import base64
import requests

# å‘é€å›¾åƒ+æ–‡æœ¬è¯·æ±‚
with open("image.jpg", "rb") as f:
    image_base64 = base64.b64encode(f.read()).decode()

response = requests.post(
    "http://localhost:8080/v1/chat/completions",
    json={
        "model": "AutoGLM-Phone-9B",
        "messages": [{
            "role": "user",
            "content": [
                {"type": "text", "text": "æè¿°è¿™å¼ å›¾ç‰‡"},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
            ]
        }]
    }
)
```

### 3. ä¸ AutoGLM é¡¹ç›®é›†æˆ

é…åˆ [AutoGLM](https://github.com/THUDM/AutoGLM) é¡¹ç›®ä½¿ç”¨ï¼Œå®ç°æ‰‹æœºè‡ªåŠ¨åŒ–æ§åˆ¶ã€‚

## ğŸ”§ æŠ€æœ¯æ¶æ„

### æ ¸å¿ƒçªç ´

#### 1. ç»´åº¦è½¬æ¢
```
GLM-4V è¾“å‡º: [HÃ—W, 6144]
    â†“ é™ç»´
llama.cpp æœŸæœ›: [HÃ—W, 4096]
```

#### 2. å±‚æ˜ å°„ä¿®å¤
```python
# GLM-4V merger ç»“æ„
{
    "norm": "mlp.0",      # Layer normalization
    "up_proj": "mlp.1",   # Up projection
    "down_proj": "mlp.3"  # Down projection
}
```

#### 3. å¯é€‰ CLS Token
```cpp
if (model.class_embedding != nullptr) {
    // æ·»åŠ  CLS token
    embeddings = ggml_concat(ctx, class_emb, embeddings, 1);
}
```

### å…³é”®ä»£ç ä¿®æ”¹

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ | è¯´æ˜ |
|------|---------|------|
| `convert_hf_to_gguf.py` | æ–°å¢ `Glm4vVisionModel` ç±» | æ”¯æŒ GLM-4V mmproj è½¬æ¢ |
| `tools/mtmd/clip.cpp` | ä¿®æ”¹ `build_internvl()` | æ”¯æŒå¯é€‰ CLS token |
| `gguf-py/gguf/constants.py` | æ·»åŠ  INTERNVL projector | GLM-4V ä½¿ç”¨æ­¤ projector ç±»å‹ |

## ğŸ“š ç›¸å…³æ–‡æ¡£

- ğŸ“– [llama.cpp æ„å»ºæŒ‡å—](docs/build.md) - ç¼–è¯‘è¯´æ˜
- ğŸ“ [å¤šæ¨¡æ€æ”¯æŒ](tools/mtmd/README.md) - mmproj ä½¿ç”¨æŒ‡å—
- ï¿½ [AutoGLM é¡¹ç›®](https://github.com/THUDM/AutoGLM) - æ‰‹æœºè‡ªåŠ¨åŒ–
- ï¿½ [GLM-4V æ¨¡å‹](https://huggingface.co/THUDM/glm-4v-9b) - Hugging Face

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### ä½¿ç”¨ llama-cli æµ‹è¯•

```bash
# æ–‡æœ¬+å›¾åƒè¾“å…¥
.\build\bin\Release\llama-cli \
  -m AutoGLM-Phone-9B-Q4_K_S.gguf \
  --mmproj mmproj-AutoGLM-Phone-9B-f16.gguf \
  -p "æè¿°è¿™å¼ å›¾ç‰‡" \
  --image screenshot.jpg
```

### ä½¿ç”¨ curl æµ‹è¯• API

```bash
# å¯åŠ¨æœåŠ¡å™¨
.\build\bin\Release\llama-server.exe \
  -m AutoGLM-Phone-9B-Q4_K_S.gguf \
  --mmproj mmproj-AutoGLM-Phone-9B-f16.gguf \
  --port 8080 -ngl 99 -c 16384

# å‘é€è¯·æ±‚ï¼ˆéœ€è¦å…ˆå°†å›¾åƒè½¬ä¸º base64ï¼‰
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4-vision-preview",
    "messages": [{
      "role": "user",
      "content": [
        {"type": "text", "text": "è¿™æ˜¯ä»€ä¹ˆï¼Ÿ"},
        {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,<BASE64>"}}
      ]
    }]
  }'
```

## ğŸ” æŠ€æœ¯ç»†èŠ‚

### æ•°æ®æµ

```
ç”¨æˆ·ä»»åŠ¡
   â†“
å®æ—¶æˆªå›¾ (ADB)
   â†“
Base64 ç¼–ç 
   â†“
Vision Encoder (mmproj)
   â†“ [HÃ—W, 6144]
ç»´åº¦é™ç»´
   â†“ [HÃ—W, 4096]
Language Model
   â†“
<think>åˆ†æ</think>
<answer>æ“ä½œ</answer>
   â†“
è§£æ + æ‰§è¡Œ (ADB)
   â†“
ç»“æœåé¦ˆ
```

### å…³é”®æŠ€æœ¯

**1. è§†è§‰ç¼–ç **
- EVA-CLIP-6B ä½œä¸ºè§†è§‰ç¼–ç å™¨
- Pixel unshuffle ç©ºé—´ä¸‹é‡‡æ ·
- å¯å­¦ä¹ çš„ä½ç½®ç¼–ç 

**2. è§†è§‰-è¯­è¨€å¯¹é½**
- MLP merger é™ç»´æŠ•å½±
- FFN (SwiGLU) æ¿€æ´»å‡½æ•°
- æ®‹å·®è¿æ¥ + Layer Norm

**3. æ¨ç†ä¼˜åŒ–**
- CUDA å›¾ä¼˜åŒ–
- Flash Attention
- KV Cache ç®¡ç†

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: llama-server å¯åŠ¨å¤±è´¥**
```bash
# æ£€æŸ¥ CUDA
nvidia-smi

# é‡æ–°ç¼–è¯‘
cd llama.cpp
.\rebuild_llama_server.ps1
```

**Q: ä¸Šä¸‹æ–‡æº¢å‡º**
```bash
# å¢åŠ ä¸Šä¸‹æ–‡
--ctx-size 16384  # æˆ–æ›´å¤§
```

**Q: ADB è¿æ¥å¤±è´¥**
```bash
# é‡å¯ ADB
adb kill-server
adb start-server

# æ— çº¿è¿æ¥
adb connect 192.168.x.x:port
```

æ›´å¤šé—®é¢˜ï¼šæŸ¥çœ‹ [Issues](https://github.com/Luckybalabala/llama-cpp-AutoGLM/issues)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿å„ç§å½¢å¼çš„è´¡çŒ®ï¼

- ğŸ› æŠ¥å‘Š Bug - é€šè¿‡ GitHub Issues
- ğŸ’¡ æå‡ºåŠŸèƒ½å»ºè®® - é€šè¿‡ GitHub Discussions
- ğŸ“ æ”¹è¿›æ–‡æ¡£ - æäº¤ Pull Request
- ğŸ”§ æäº¤ä»£ç  - Fork åæäº¤ PR

æŸ¥çœ‹ä¸Šæ¸¸ [llama.cpp è´¡çŒ®æŒ‡å—](CONTRIBUTING.md) äº†è§£ä»£ç è§„èŒƒã€‚

### è´¡çŒ®è€…

æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT è®¸å¯è¯](LICENSE)ã€‚

## ğŸ™ è‡´è°¢

- [llama.cpp](https://github.com/ggerganov/llama.cpp) - ä¼˜ç§€çš„æ¨ç†æ¡†æ¶
- [AutoGLM](https://github.com/THUDM/AutoGLM) - åŸå§‹æ¨¡å‹
- [GLM-4V](https://github.com/THUDM/GLM-4) - è§†è§‰è¯­è¨€æ¨¡å‹

## ğŸ“ è”ç³»æ–¹å¼

- **Issues**: [GitHub Issues](https://github.com/Luckybalabala/llama-cpp-AutoGLM/issues)
- **Discussions**: [GitHub Discussions](https://github.com/Luckybalabala/llama-cpp-AutoGLM/discussions)
- **å¾®ä¿¡ç¾¤**: [åŠ å…¥è®¨è®º](resources/WECHAT.md)

## ğŸ—ºï¸ å®ç°çŠ¶æ€

### âœ… å·²å®Œæˆ
- [x] GLM-4V è¯­è¨€æ¨¡å‹ GGUF è½¬æ¢
- [x] GLM-4V è§†è§‰ç¼–ç å™¨ mmproj è½¬æ¢
- [x] InternVL projector æ¶æ„é€‚é…
- [x] å¯é€‰ CLS token æ”¯æŒ
- [x] 16K ä¸Šä¸‹æ–‡çª—å£æ”¯æŒ
- [x] OpenAI å…¼å®¹ API

### ï¿½ æŠ€æœ¯å®ç°
- **Projector Type**: INTERNVL
- **Vision Encoder**: EVA-CLIP-6B
- **Text Model**: GLM-4-9B
- **Context Length**: 16,384 tokens
- **Output Dimension**: 4096 (ä» 6144 é™ç»´)

## ğŸ“ˆ é¡¹ç›®çŠ¶æ€

![GitHub stars](https://img.shields.io/github/stars/Luckybalabala/llama-cpp-AutoGLM?style=social)
![GitHub forks](https://img.shields.io/github/forks/Luckybalabala/llama-cpp-AutoGLM?style=social)
![GitHub issues](https://img.shields.io/github/issues/Luckybalabala/llama-cpp-AutoGLM)

---

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼**

**ğŸ‰ è¿™æ˜¯ä¸–ç•Œé¦–ä¸ª GLM-4V GGUF å®ç°ï¼Œè®©æˆ‘ä»¬ä¸€èµ·æ¨åŠ¨å¼€æºå¤šæ¨¡æ€ AI çš„å‘å±•ï¼**
